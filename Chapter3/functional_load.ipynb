{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "functional_load.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5NXPjrYO7Ws"
      },
      "source": [
        "This script contains the basic functions needed to perform functional load calculations.\n",
        "\n",
        "In this case, we apply the functions to the classic [Surendran and Niyogi (2006)](http://people.cs.uchicago.edu/~dinoj/fload_bookchapter.pdf) 's toy example.\n",
        "\n",
        "\n",
        "\n",
        "#1. Introduction\n",
        "First, we import the standard modules Counter and Math."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2K2oxsYPiFY"
      },
      "source": [
        "from collections import Counter\n",
        "import math"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJqQ-M8IPkFC"
      },
      "source": [
        "Then, we encode the toy example in Surendran and Niyogi (2006) as a string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_4cDMHyOyZt"
      },
      "source": [
        "toy = 'atuattatuatatautuaattuua'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUd8IlBXpqwZ"
      },
      "source": [
        "#2. Extract ngrams\n",
        "\n",
        "Now, we need a function to extract ngrams given a parameter k."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvoEKZ1up-VX"
      },
      "source": [
        "def ngrams(text, k=1):\n",
        "    '''\n",
        "    :param text: the input text\n",
        "    :param k: the order of the Markov model\n",
        "    :return: ngram counts\n",
        "    '''\n",
        "    counts = Counter()\n",
        "    if k == 0: #return unigrams if k=0\n",
        "        counts = Counter(text)\n",
        "    else: #return k+1grams if k>1\n",
        "        for index, letter in enumerate(text[:-k]):\n",
        "            counts[text[index:index+k+1]] +=1\n",
        "    return counts\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjwou6pfqC3U"
      },
      "source": [
        "By applying the function to the text with k=1, we obtain the phoneme bigram distribution presented in Surendran and Niyogi (2006)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyxUeodvqtqT",
        "outputId": "9d7b872d-ee43-42f3-86ac-c23ef57ae9a5"
      },
      "source": [
        "for bigram, count in ngrams(toy).items():\n",
        "  print(bigram,count)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "at 6\n",
            "tu 4\n",
            "ua 4\n",
            "tt 2\n",
            "ta 3\n",
            "au 1\n",
            "ut 1\n",
            "aa 1\n",
            "uu 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18a34fD6rDnN"
      },
      "source": [
        "#3. Calculate entropy\n",
        "\n",
        "Now that we have an ngram distribution, we can calculate entropy using the classic Shannon formula:\n",
        "\n",
        "<a href=\"https://www.codecogs.com/eqnedit.php?latex=H_{kS}(L)&space;=&space;\\frac{1}{k&plus;1}&space;\\left(-\\sum_{x&space;\\in&space;X}^{}&space;p(x)log_{2}p(x)\\right)\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?H_{kS}(L)&space;=&space;\\frac{1}{k&plus;1}&space;\\left(-\\sum_{x&space;\\in&space;X}^{}&space;p(x)log_{2}p(x)\\right)\" title=\"H_{kS}(L) = \\frac{1}{k+1} \\left(-\\sum_{x \\in X}^{} p(x)log_{2}p(x)\\right)\" /></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEkM9BgErAlp"
      },
      "source": [
        "def entropy(text, k=1):\n",
        "    '''\n",
        "    :param text: the input text\n",
        "    :param k: the order of the Markov model\n",
        "    :return: entropy\n",
        "    '''\n",
        "    ngrams_dic = ngrams(text, k) #retrieves ngrams\n",
        "    total = sum(ngrams_dic.values()) #ngram total\n",
        "    sommation = 0\n",
        "    for value in ngrams_dic.values(): #sommation\n",
        "        sommation += value/total * math.log(value/total, 2)\n",
        "    sommation = sommation / (k+1)\n",
        "    return -sommation\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxbKduCHr3U7"
      },
      "source": [
        "We can now calculate the entropy of our toy example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4e-hcf1r_AG",
        "outputId": "b419ea01-d831-494b-e8b4-5f02ff2295e0"
      },
      "source": [
        "print('The entropy of the text is %s' % round(entropy(toy),3))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The entropy of the text is 1.43\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5yTP8SPrhoK"
      },
      "source": [
        "#4. Estimate functional load\n",
        "\n",
        "Through the functional load formula defined by Hockett (1955):\n",
        "\n",
        "<a href=\"https://www.codecogs.com/eqnedit.php?latex=FL(x,y)&space;=&space;\\frac{H(L)&space;-&space;H(L_{xy})}{H(L)}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?FL(x,y)&space;=&space;\\frac{H(L)&space;-&space;H(L_{xy})}{H(L)}\" title=\"FL(x,y) = \\frac{H(L) - H(L_{xy})}{H(L)}\" /></a>\n",
        "\n",
        "We can estimate the functional load of every contrast *(x,y)* in the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0WJcUcAruqj"
      },
      "source": [
        "def functional_load(text, phon1, phon2):\n",
        "    '''\n",
        "    :param text: the input text\n",
        "    :param phon1: phoneme replaced\n",
        "    :param phon2: phoneme used as replacement\n",
        "    :return: the different in entropy between the two states\n",
        "    '''\n",
        "    merged_text = text.replace(phon1, phon2)\n",
        "    return (entropy(text)-entropy(merged_text))/entropy(text)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEoekdUfsTJA"
      },
      "source": [
        "Now we can estimate entropy loss for each contrast."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pdT9Jgrsd-q",
        "outputId": "d092b070-c094-4784-b7ea-13b98ded22a3"
      },
      "source": [
        "print('Entropy loss after a merger between [a] and [u] is %s' % round(functional_load(toy, 'a', 'u'),3))\n",
        "print('Entropy loss after a merger between [a] and [t] is %s' % round(functional_load(toy, 'a', 't'),3))\n",
        "print('Entropy loss after a merger between [t] and [u] is %s' % round(functional_load(toy, 't', 'u'),3))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Entropy loss after a merger between [a] and [u] is 0.345\n",
            "Entropy loss after a merger between [a] and [t] is 0.425\n",
            "Entropy loss after a merger between [t] and [u] is 0.381\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}